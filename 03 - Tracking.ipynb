{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow import keras\n",
    "import cv2\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = 12, 8\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"dense\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               3211392   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 3,219,973\n",
      "Trainable params: 0\n",
      "Non-trainable params: 3,219,973\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.applications import VGG16\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Input, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.metrics import Recall\n",
    "\n",
    "vgg = VGG16(include_top=False, weights=\"imagenet\")\n",
    "vgg.trainable = False\n",
    "\n",
    "dense = load_model(\"modele-VGG16\") # ou 'modele-VGG16.h5'\n",
    "modele = Sequential([\n",
    "    Input((227, 227, 3)),\n",
    "    vgg,\n",
    "    dense\n",
    "], name=\"complet\")\n",
    "modele.trainable = False\n",
    "\n",
    "dense.summary()\n",
    "recall = Recall(name=\"recall\")\n",
    "modele.compile('adam', 'categorical_crossentropy', metrics=[recall])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "NOM = \"BowlPlace1Subject1\"\n",
    "VIDEO = join(\"VIDEOS\", NOM + \".mp4\")\n",
    "BOITE = join(\"GT\", NOM + \"_2_bboxes.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture des boites fournies\n",
    "Lit le fichier contenant les informations de reconnaissance d'objets pour avoir un point de départ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import exists, join\n",
    "from typing import List, Tuple\n",
    "import csv\n",
    "\n",
    "def lecture_information_boite_englobante(nom: str) -> List[List[Tuple[int, int, int, int]]]:\n",
    "    \"\"\" Lit les boites englobante fournies pour l'exemple. \"\"\"\n",
    "    resultats = list()\n",
    "    \n",
    "    with open(join(\"GT\", nom + \"_2_bboxes.txt\"), \"r\") as fichier:\n",
    "        lecteur = csv.reader(fichier, delimiter=\" \")\n",
    "\n",
    "        for ligne in lecteur:\n",
    "            frame_nb = int(ligne[0])\n",
    "            boite_nb = int(ligne[1])\n",
    "            boites = list()\n",
    "            for i in range(boite_nb):\n",
    "                boite = (int(ligne[i * 4 + 2]), int(ligne[i * 4 + 3]), int(ligne[i * 4 + 4]), int(ligne[i * 4 + 5]))\n",
    "                boites.append(boite)\n",
    "            resultats.append(boites)\n",
    "    \n",
    "    return resultats\n",
    "\n",
    "boites_englobantes = lecture_information_boite_englobante(NOM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recherche de l'objet sur la prochaine image\n",
    "À partir de la zone désignée dans l'image précédante, tente de retrouver l'objet dans la nouvelle image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction `evaluation_boite` prend en entrée une liste de boite (x, y, hauteur, largeur) et retourne l’évalution de chacune d’elle par le réseau de neurones.\n",
    "L’évalution se fait par groupes de boites pour aller plus vite.\n",
    "\n",
    "La fonction `tracking_move_box` tente de repositionner la boite en explorant les alentours et en raffinant les déplacements.\n",
    "La longueur du _pas_ de déplacement diminue à chaque itération et l’exploration recommence à proximité de la meilleure position connue.\n",
    "Cet algorithme recherche une maximum local avant de raffiner sa position.\n",
    "\n",
    "Il n'est pas performant quand le maximum atteind la valeur 1.\n",
    "En effet le réseau est entraîné à reconnaitre les objets même incomplet et peut obtenir une grande certitude sans avoir l'image dans son intégralité."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def evaluation_boite(image: Image, boites: list) -> np.array:\n",
    "    \"\"\" Évalue les boites englobantes.\n",
    "    Passer plusieurs boites englobantes en même temps permet d'accélérer le calcul.\n",
    "    \"\"\"\n",
    "    #print(f\"evaluation de {len(boites)} boites\")\n",
    "    extraits = list(np.asarray(image\\\n",
    "        .resize((227, 227), box=((boite[0], boite[1], boite[0] + boite[2], boite[1] + boite[3]))))\n",
    "        for boite in boites)\n",
    "    return modele.predict(np.array(extraits))\n",
    "\n",
    "def tracking_move_box(image: Image.Image, boite_pre: (int, int, int, int), categorie: int) -> (int, int, int, int):\n",
    "    \"\"\"Calcul le mouvement de la boite.\n",
    "    @param image: image courante\n",
    "    @param boite_pre: ancienne boite englobante\n",
    "    @return la nouvelle position de la boite englobante\n",
    "    \"\"\"\n",
    "    assert(isinstance(boite_pre, tuple)), type(boite_pre)\n",
    "    assert isinstance(boite_pre[0], int) and isinstance(boite_pre[1], int) and isinstance(boite_pre[2], int)\n",
    "    \n",
    "    valeurs = evaluation_boite(image, (boite_pre,))\n",
    "    evaluations = {boite_pre: valeurs[0][categorie]}  # categorie\n",
    "    meilleur_pos = boite_pre\n",
    "    #print(meilleur_pos, valeur[categorie])\n",
    "    #print(boite_cible, valeurs[1][categorie])\n",
    "\n",
    "    # Modifications légères de la boite en maximisant la correspondance à la catégorie.\n",
    "    # la déplacer un peu et réessayer\n",
    "    # Méthode diamand sur 4D (x, y, w, h) ou (x, y, ratio, zoom)\n",
    "    pas = 32\n",
    "    while pas >= 8:\n",
    "        # Liste des positions évaluées mais pas leurs voisins\n",
    "        exploration = [meilleur_pos]\n",
    "        # Liste des positions évaluées et aussi leurs voisins\n",
    "        explores = set()\n",
    "\n",
    "        while len(exploration) > 0 and (len(explores) < 4 or \\\n",
    "            max(explores, key=evaluations.__getitem__) < max(exploration, key=evaluations.__getitem__)):\n",
    "            # Une position plus favorable nécessite d'étudier ses voisins\n",
    "            pos = max(exploration, key=evaluations.__getitem__)\n",
    "            exploration.remove(pos)\n",
    "\n",
    "            # propositions de nouvelles boites englobantes\n",
    "            propositions = list(boite_v\n",
    "                for boite_v in boites_voisines(pos, image.size, pas=pas)\n",
    "                    if boite_v not in evaluations.keys())\n",
    "            propositions = list(boite_v for boite_v in propositions if boite_v not in evaluations)\n",
    "\n",
    "            # évalue toutes les boites proposées\n",
    "            valeurs = evaluation_boite(image, propositions)\n",
    "            for i, boite_v in enumerate(propositions):\n",
    "                # note pour la catégorie considérée\n",
    "                evaluations[boite_v] = valeurs[i][categorie]\n",
    "\n",
    "            exploration.extend(propositions)\n",
    "            explores.add(pos)\n",
    "\n",
    "        meilleur_pos = max(explores, key=evaluations.__getitem__)\n",
    "        print(f\"\\tpas={pas}, pos={meilleur_pos}, valeur={evaluations[meilleur_pos]}\")\n",
    "        pas = int(pas / 2)\n",
    "    return meilleur_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette version du tracking est plus légère et ne réalise qu’une seule exploration des environs.\n",
    "Toutes les boites candidates sont évaluées d'un seul coup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tracking_leger(image: Image.Image, boite_pre: (int, int, int, int), categorie: int) -> (int, int, int, int):\n",
    "    pas = 2\n",
    "    demi_taille = 5\n",
    "    positions = list()\n",
    "    evaluations = {}\n",
    "    positions.append(boite_pre)\n",
    "    for dx in range(-demi_taille, demi_taille, pas):\n",
    "        for dy in range(-demi_taille, demi_taille, pas):\n",
    "            positions.append((boite_pre[0] + dx, boite_pre[1] + dy, boite_pre[2], boite_pre[3]))\n",
    "    valeurs = evaluation_boite(image, positions)\n",
    "    for i, boite_v in enumerate(positions):\n",
    "        evaluations[boite_v] = valeurs[i][categorie]\n",
    "    return max(positions, key=evaluations.__getitem__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracking sur vidéo\n",
    "Cette boucle doit retrouver la position de la boite englobante en fonction de sa dernière position.\n",
    "\n",
    "Utiliser n'importe quelle touche pour fermer la fenêtre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.evaluateBBox import *\n",
    "\n",
    "assert exists(VIDEO), \"le fichier vidéo n'existe pas\"\n",
    "PAS_FRAME = 5 # ignore une partie des frames\n",
    "DEPART = 0\n",
    "\n",
    "frame = 0\n",
    "video = cv2.VideoCapture(VIDEO)\n",
    "tracking = False\n",
    "categorie = 0\n",
    "sequence = list()\n",
    "\n",
    "while video.isOpened():\n",
    "    # print(f\"* frame {frame}\")\n",
    "    ret, f = video.read()\n",
    "    if ret == False:\n",
    "        print(\"erreur\")\n",
    "        break\n",
    "\n",
    "    f = cv2.cvtColor(f, cv2.COLOR_BGR2RGB)\n",
    "    image = Image.fromarray(f)\n",
    "        \n",
    "    # découper une boite\n",
    "    if not tracking and len(boites_englobantes[frame]) > 0:\n",
    "        tracking = True\n",
    "        sequence.append(boites_englobantes[frame][0])\n",
    "        valeurs = evaluation_boite(image, (sequence[-1],))\n",
    "        categorie = np.argmax(valeurs[0])\n",
    "        DEPART = frame % PAS_FRAME\n",
    "        print(\"tracking pour la catégorie\", categorie)\n",
    "        \n",
    "    elif tracking and frame % PAS_FRAME == DEPART:\n",
    "        boite_pre = sequence[-PAS_FRAME]\n",
    "        boite_cible = boites_englobantes[frame][0]\n",
    "        print(f\"boite précédante: {boite_pre}, boite cible: {boite_cible}\")\n",
    "        \n",
    "        meilleur_pos = tracking_move_box(image, boite_pre, categorie)\n",
    "        # meilleur_pos = tracking_leger(image, boite_pre, categorie)\n",
    "\n",
    "        print(f\"ajustement de {boite_pre} à {meilleur_pos}\")\n",
    "        print(f\"score: {IoU(boite_cible, meilleur_pos)}\")\n",
    "        sequence.append(meilleur_pos)\n",
    "        x, y, w, h = meilleur_pos\n",
    "\n",
    "        ff = f.copy()\n",
    "        cv2.rectangle(ff, (x, y), (x+w, y+h), (0, 255, 0), 2) \n",
    "        cv2.imshow(\"frame \"+str(frame), ff)\n",
    "        cv2.waitKey(30000)\n",
    "        cv2.destroyAllWindows()\n",
    "        \n",
    "    else:\n",
    "        sequence.append(None)\n",
    "    frame += 1\n",
    "\n",
    "print(\"out\")\n",
    "video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
